{
  "slug": "building-a-gemini-chat-app",
  "title": "Building a Secure Gemini AI Chat App with Next.js App Router",
  "date": "2025-08-11",
  "description": "A step-by-step guide on how I built a secure, end-to-end chat application using Next.js, TypeScript, and the Gemini API, with a focus on data flow and security.",
  "category": "Dev Log",
  "content": "## Building a Gemini AI Chat App: A Step-by-Step Guide\n\nToday, I built a secure, end-to-end chat application using Next.js and the Gemini API. The goal was to understand the fundamental architecture of integrating AI into a web app, focusing on security and maintainability. I broke the project into eight manageable steps, from initial setup to final UI polish. Here's a summary of the journey.\n\n### Step 1: Project Setup\n\nI started by creating a new Next.js project using the App Router. This provided a modern foundation with TypeScript for type safety and ESLint for code quality. The command `pnpm create next-app my-gemini-chat` with the App Router and TypeScript options gave me a clean, well-structured project to start with.\n\n### Step 2: Secure API Key Storage\n\nTo keep my Gemini API key private, I used a `.env.local` file. This file is automatically loaded by Next.js on the server side and is excluded from version control by the `.gitignore` file, ensuring the key is never exposed. The single line `GEMINI_API_KEY=YOUR_API_KEY` was all that was needed to securely store my key.\n\n### Step 3: Minimal UI Design\n\nWith the project configured, I built the user interface. Using `app/page.tsx`, I created a simple chat layout with a message list, a text input, and a send button. I used plain CSS in `app/globals.css` to style the messages and layout, making it functional and readable without relying on a UI framework.\n\n### Step 4: Client-Side Logic\n\nI made the UI interactive by adding client-side logic. Using the `\"use client\";` directive and React's `useState` hook, I managed the chat messages and input field state. A `handleSubmit` function was created to handle form submissions, updating the message list and clearing the input field, which made the app feel responsive.\n\n### Step 5: Creating the Server API Route\n\nI created a secure API endpoint at `app/api/chat/route.ts`. This endpoint, a Next.js App Router convention, serves as a proxy between the client and the Gemini API. The `POST` function in this file initially logged the client's message and sent back a hard-coded response, confirming the server-side route was working correctly.\n\n### Step 6: Connecting Client and Server\n\nI connected the client to the server by updating the `handleSubmit` function. It now uses the `fetch` API to send a `POST` request to `/api/chat`. The client waits for the response from the server and then adds it to the chat message state, completing the communication loop.\n\n### Step 7: Integrating the Gemini SDK\n\nThis was the core integration step. After installing the `@google/generative-ai` package, I modified the server API route (`app/api/chat/route.ts`). I initialized the Gemini client using the secure `GEMINI_API_KEY` from `process.env` and called the `gemini-1.5-flash` model with the user's message. The model's response was then sent back to the client.\n\n**Example code from `app/api/chat/route.ts`:**\n\n```ts\nimport { NextResponse } from 'next/server';\nimport { GoogleGenerativeAI } from '@google/generative-ai';\n\nconst apiKey = process.env.GEMINI_API_KEY;\n\nif (!apiKey) {\n  throw new Error('GEMINI_API_KEY environment variable is not set.');\n}\n\nconst genAI = new GoogleGenerativeAI(apiKey);\n\nexport async function POST(req: Request) {\n  const { message } = await req.json();\n  const model = genAI.getGenerativeModel({ model: 'gemini-1.5-flash' });\n  const result = await model.generateContent(message);\n  const text = result.response.text();\n  return NextResponse.json({ role: 'ai', content: text });\n}\n```\n\n### Step 8: Adding UI Feedback\n\nFinally, I improved the user experience by adding loading and error states. I used `useState` hooks for `isLoading` and `error` and integrated them into the `handleSubmit` function using a `try/catch/finally` block. This allowed the UI to display a loading animation while waiting for a response and a clear error message if something went wrong.\n\n### Data Flow and Architecture Overview\n\nThis architecture follows a **client-server proxy model**. The data flow is a secure round trip:\n\n1. **Client** -> `fetch` POST request -> **Server API Route**\n2. **Server API Route** -> uses `GEMINI_API_KEY` to call -> **Gemini API**\n3. **Gemini API** -> sends response -> **Server API Route**\n4. **Server API Route** -> sends `NextResponse` back -> **Client**\n\nThis approach ensures that the sensitive API key is never exposed in the browser, making the application secure and robust.  The project successfully demonstrates a scalable and secure way to integrate powerful AI models into web applications."
}